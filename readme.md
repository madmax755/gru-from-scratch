# Learning C++ via Gated Recurrent Unit Neural Network Implementation

GRU neural network architecture implemented from scratch in C++ for learning purposes.

- Custom implementation for sequence processing without ML libraries
- Features:
  - GRU cell implementation
  - MLP output layer
  - Configurable input, hidden, and output dimensions
  - AdamW optimiser
  - Financial metrics for stock prediction (profit/loss, directional accuracy)

```text
├── GRU/
│   ├── gru.cpp          # Basic GRU implementation
│   ├── gru+mlp.cpp      # GRU with MLP output layer
│   └── stock_data/      # Financial data for testing
```

## Learning Outcomes
- See `learnings.md` for detailed C++ insights gained during implementation.
- Understanding of NN architectures, how they work, and how to train them.

## Requirements
- C++ compiler with C++17 support
- Standard Template Library (STL)


